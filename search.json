[
  {
    "objectID": "mediators.html",
    "href": "mediators.html",
    "title": "Mediators",
    "section": "",
    "text": "Mediators\n\n\n\nd3 = require(\"d3@7\")\ndag = import(new URL(\"js/dag-utils.js\", document.baseURI).href)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships between nodes\n\nviewof strength_xz = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-z\"&gt;Z&lt;/span&gt; strength`\n})\n\nviewof strength_zy = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-z\"&gt;Z&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; strength`\n})\n\nviewof strength_xy = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; strength`\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustments\n\nviewof adjust_z = Inputs.toggle({\n  label: html`&lt;span class=\"node node-z\"&gt;Adjust for Z&lt;/span&gt; (&lt;em&gt;block indirect path&lt;/em&gt;)`\n})\n\n\n\n\n\n\n\n// ----------------\n// Status readout\n// ----------------\n{\n  const pctDirect = Math.round(y_direct_x / yMax * 100);\n  const pctMediated = Math.round(y_mediated / yMax * 100);\n  const pctZ = Math.round(y_from_z_own / yMax * 100);\n  const pctOwn = Math.max(0, 100 - pctDirect - pctMediated - pctZ);\n\n  return html`&lt;div class=\"alert alert-secondary status-readout\"&gt;\n    &lt;h5 class=\"alert-heading\"&gt;What Y contains&lt;/h5&gt;\n    &lt;table&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorX}\"/&gt;&lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-x\"&gt;X&lt;/span&gt;'s direct influence on &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt;&lt;/td&gt;\n        &lt;td&gt;${pctDirect}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;\n          &lt;defs&gt;\n            &lt;pattern id=\"legend-hatch-med\" patternUnits=\"userSpaceOnUse\"\n              width=\"6\" height=\"6\" patternTransform=\"rotate(-45)\"&gt;\n              &lt;rect width=\"6\" height=\"6\" fill=\"${dag.colorX}\"/&gt;\n              &lt;line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"6\"\n                stroke=\"${dag.colorZ}\" stroke-width=\"2.5\"/&gt;\n            &lt;/pattern&gt;\n          &lt;/defs&gt;\n          &lt;rect width=\"12\" height=\"12\" fill=\"url(#legend-hatch-med)\"/&gt;\n        &lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-x\"&gt;X&lt;/span&gt;'s influence on &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; via &lt;span class=\"node node-z\"&gt;Z&lt;/span&gt;&lt;/td&gt;\n        &lt;td&gt;${pctMediated}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorZ}\"/&gt;&lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-z\"&gt;Z&lt;/span&gt;'s own variation flowing to &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt;&lt;/td&gt;\n        &lt;td&gt;${pctZ}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorY}\"/&gt;&lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-y\"&gt;Y&lt;/span&gt;'s own variation&lt;/td&gt;\n        &lt;td&gt;${pctOwn}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr class=\"summary ${adjust_z ? 'dimmed' : ''}\"&gt;\n        &lt;td&gt;\n          &lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorX}\"/&gt;&lt;/svg&gt;\n          +\n          &lt;svg width=\"12\" height=\"12\"&gt;\n            &lt;rect width=\"12\" height=\"12\" fill=\"url(#legend-hatch-med)\"/&gt;\n          &lt;/svg&gt;\n        &lt;/td&gt;\n        &lt;td&gt;Total &lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; effect&lt;/td&gt;\n        &lt;td&gt;${pctDirect + pctMediated}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr class=\"summary ${adjust_z ? '' : 'dimmed'}\"&gt;\n        &lt;td&gt;\n          &lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorX}\"/&gt;&lt;/svg&gt;\n        &lt;/td&gt;\n        &lt;td&gt;Direct-only &lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; effect&lt;/td&gt;\n        &lt;td&gt;${pctDirect}%&lt;/td&gt;\n      &lt;/tr&gt;\n    &lt;/table&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\n\n\n\n\nyMax = 150\nbaseVal = 50\n\nz_from_x = strength_xz * baseVal\n\ny_direct_x = strength_xy * baseVal\ny_mediated = adjust_z ? 0 : strength_zy * z_from_x\ny_from_z_own = adjust_z ? 0 : strength_zy * (1 - strength_xz) * baseVal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// -----------------\n// Interactive DAG\n// -----------------\n{\n  const width = 600;\n  const height = 250;\n  const nodeRadius = 36;\n\n  // Mediator: Z is between X and Y, positioned at top\n  const nodes = {\n    X: { x: 130, y: 200, label: \"X\" },\n    Z: { x: width / 2, y: 60, label: \"Z\" },\n    Y: { x: 470, y: 200, label: \"Y\" }\n  };\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", `0 0 ${width} ${height}`)\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .style(\"max-width\", \"100%\");\n\n  const defs = svg.append(\"defs\");\n\n  dag.addArrowMarkers(defs);\n\n  // Mediation hatch: red stripes on gold, slope downward\n  // \"X flowing through Z's territory\"\n  dag.addHatchPattern(\n    defs, \"hatch-mediated\", dag.colorX, dag.colorZ, -45\n  );\n\n  dag.addCircleClip(\n    defs, \"z-clip\", nodes.Z.x, nodes.Z.y, nodeRadius\n  );\n  dag.addCircleClip(\n    defs, \"y-clip\", nodes.Y.x, nodes.Y.y, nodeRadius\n  );\n\n  // Arrows\n  const edges = [\n    {\n      id: \"xz\", from: nodes.X, to: nodes.Z,\n      strength: strength_xz, blocked: false\n    },\n    {\n      id: \"zy\", from: nodes.Z, to: nodes.Y,\n      strength: strength_zy, blocked: adjust_z\n    },\n    {\n      id: \"xy\", from: nodes.X, to: nodes.Y,\n      strength: strength_xy, blocked: false\n    }\n  ];\n\n  for (const edge of edges) {\n    dag.drawEdge(svg, edge, nodeRadius);\n  }\n\n  // Nodes\n  // X is solid\n  dag.drawSolidNode(\n    svg, nodes.X.x, nodes.X.y, nodeRadius, dag.colorX\n  );\n\n  // Z: strength directly controls fill proportion\n  dag.drawNode(svg, nodes.Z.x, nodes.Z.y, nodeRadius, \"z-clip\", {\n    bottomUp: [\n      { prop: strength_xz, fill: dag.colorX }\n    ],\n    topDown: []\n  }, \"horizontal\", dag.colorZ);\n\n  // Y: blue base, incoming effects overlay\n  dag.drawNode(svg, nodes.Y.x, nodes.Y.y, nodeRadius, \"y-clip\", {\n    bottomUp: [\n      { prop: Math.min(y_direct_x / yMax, 1), fill: dag.colorX },\n      {\n        prop: Math.min(y_mediated / yMax, 1),\n        fill: \"url(#hatch-mediated)\"\n      }\n    ],\n    topDown: [\n      {\n        prop: Math.min(y_from_z_own / yMax, 1),\n        fill: dag.colorZ\n      }\n    ]\n  }, undefined, dag.colorY);\n\n  // Labels\n  for (const n of Object.values(nodes)) {\n    dag.drawLabel(svg, n.x, n.y, n.label);\n  }\n\n  return svg.node();\n}"
  },
  {
    "objectID": "do-calculus.html",
    "href": "do-calculus.html",
    "title": "do-calculus",
    "section": "",
    "text": "do(•)\nTODO: Add citations to Pearl and others from my blog post\nPearl’s do-calculus provides a set of algebraic rules for eliminating interventional \\operatorname{do}(\\cdot) operators from causal expressions, allowing us to estimate causal effects from observational data."
  },
  {
    "objectID": "do-calculus.html#important-notation-for-graph-surgery",
    "href": "do-calculus.html#important-notation-for-graph-surgery",
    "title": "do-calculus",
    "section": "Important notation for graph surgery",
    "text": "Important notation for graph surgery\nEach do-calculus rule works by checking if specific nodes are d-separated in modified versions of the original DAG.\nThere’s a whole set of special notation that gets used to make these temporarily modified graphs. I like to think of these as surgical instructions that tell us to cut specific arrows into or out of nodes.\n\nGraphs\nThe first bit of notation is easy. A DAG is defined as a graph G. Phew. See Figure 1 (a) for an example DAG.\n\n\nOverlines and underlines\nThe next bit of notation is new and involves adding lines above and below node names, like \\overline{X} and \\underline{X}.\nI imagine this line notation like a wall:\n\nIf the wall is on top of X like \\overline{X}, you can’t draw any arrows going into it, so you delete any arrows going in to it\nIf the wall is on the bottom of X like \\underline{X}, you can’t draw any arrows going out of it, so you delete any arrows going out of it\n\nThis line notation is added as a subscript to G:\n\nG_{\\overline{X}}: G with all arrows going into X deleted; see Figure 1 (b)\nG_{\\underline{X}}: G with all arrows going out of X deleted; see Figure 1 (c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original DAG, G\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) G_{\\overline{X}} with arrows into X removed\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) G_{\\underline{X}} with arrows out of X removed\n\n\n\n\n\n\n\n\nFigure 1: G, G_{\\overline{X}}, and G_{\\underline{X}}\n\n\n\nThese modifications can also be combined. The full rules of do-calculus use graphs like G_{\\overline{X}\\underline{Z}} (arrows into X and out of Z both removed) and G_{\\overline{X}\\overline{Z}} (arrows into X and into Z both removed). Figure 2 shows these with a four-node DAG:\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original DAG, G\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) G_{\\overline{X}\\underline{Z}}: arrows into X and out of Z removed\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) G_{\\overline{X}\\overline{Z}}: arrows into X and into Z removed\n\n\n\n\n\n\n\n\nFigure 2: Combined graph modifications with a four-node DAG\n\n\n\n\n\nAncestors\nThere’s one final bit of notation that lets us refer to ancestors of nodes—like in A \\rightarrow B, A is an ancestor or predecessor of B.\nWe use Z(W) to refer to any Z nodes that aren’t ancestors of W in G_{\\overline{X}}. In some applications of do-calculus rules, we need to look at a modified graph like G_{\\overline{X}\\overline{Z(W)}}. Instead of removing incoming arrows to every node in Z like we would with something like G_{\\overline{Z}}, we only cut arrows for Z nodes that are not ancestors of any W node in G_{\\overline{X}}. Nodes in Z that are ancestors of W in G_{\\overline{X}} keep their incoming arrows intact—severing them would alter the distribution of W, distorting the conditional we depend on.\nFigure 3 illustrates this with two Z nodes. Z_1 causes W directly, so in G_{\\overline{X}} it is an ancestor of W and is excluded from Z(W). Z_2 has no path to W in G_{\\overline{X}}, so it is included in Z(W) and its incoming arrows are removed.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original DAG, G\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) G_{\\overline{X}\\overline{Z(W)}} with Z(W) = \\{Z_2\\}: incoming arrows to X and Z_2 removed, but Z_1’s incoming arrows stay\n\n\n\n\n\n\n\n\nFigure 3: Z_1 is an ancestor of W in G_{\\overline{X}} (via Z_1 \\rightarrow W), so Z_1 \\notin Z(W) and A \\rightarrow Z_1 stays. Z_2 has no path to W, so Z_2 \\in Z(W) and A \\rightarrow Z_2 is removed."
  },
  {
    "objectID": "do-calculus.html#the-rules-of-do-calculus",
    "href": "do-calculus.html#the-rules-of-do-calculus",
    "title": "do-calculus",
    "section": "The rules of do-calculus",
    "text": "The rules of do-calculus\n\nFormal rules\nWith this special notation for graph analysis, we can finally look at the three rules of do-calculus. In their most general form, they apply to any disjoint sets of variables X, Y, Z, and W in a causal DAG G:\n\nRule 1 (Insertion/deletion of observations) An observed variable Z can be added to or removed from a conditional if Y and Z are d-separated given X and W in G_{\\overline{X}}:\n\nP(y \\mid \\operatorname{do}(x), z, w) = P(y \\mid \\operatorname{do}(x), w) \\quad \\text{if } (Y \\perp Z \\mid X, W)_{G_{\\overline{X}}}\n\n\n\nRule 2 (Action/observation exchange) An intervention \\operatorname{do}(z) can be replaced with an observation z (or vice versa) if Y and Z are d-separated given X and W in G_{\\overline{X}\\underline{Z}}:\n\nP(y \\mid \\operatorname{do}(x), \\operatorname{do}(z), w) = P(y \\mid \\operatorname{do}(x), z, w) \\quad \\text{if } (Y \\perp Z \\mid X, W)_{G_{\\overline{X}\\underline{Z}}}\n\n\n\nRule 3 (Insertion/deletion of actions) An intervention \\operatorname{do}(z) can be dropped entirely if Y and Z are d-separated given X and W in G_{\\overline{X}\\overline{Z(W)}}:\n\nP(y \\mid \\operatorname{do}(x), \\operatorname{do}(z), w) = P(y \\mid \\operatorname{do}(x), w) \\quad \\text{if } (Y \\perp Z \\mid X, W)_{G_{\\overline{X}\\overline{Z(W)}}}\n\nwhere Z(W) is the set of Z nodes that are not ancestors of any node in W in G_{\\overline{X}}.\n\nIn all three rules, X, Y, Z, and W are generic placeholders for any disjoint sets of variables—X represents existing interventions, Z is the variable being added, removed, or exchanged, Y is the outcome, and W is an optional conditioning set. Any of these sets may be empty.\n\n\nSimplified rules for single-confounder DAGs\nFor the common case of a single intervention on X with a single outcome Y and no additional conditioning (W = \\varnothing), Rules 1–3 can reduce to simpler forms. In these versions, the variable names refer to specific nodes in the DAG rather than generic placeholders:\n\nSimplified Rule 1 (Ignore an observation) We can ignore an extra observed variable Z if it is d-separated from Y in G_{\\overline{X}} (the graph with arrows into X removed):\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid \\operatorname{do}(x)) \\quad \\text{if } (Y \\perp Z \\mid X)_{G_{\\overline{X}}}\n\n\n\nSimplified Rule 2 (Treat an intervention as an observation) We can replace \\operatorname{do}(x) with simply observing x if, after removing X’s outgoing edges, X and Y are d-separated given the covariates Z:\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid x, z) \\quad \\text{if } (Y \\perp X \\mid Z)_{G_{\\underline{X}}}\n\n\n\nSimplified Rule 3 (Ignore an intervention) We can drop \\operatorname{do}(x) entirely if, after removing X’s incoming edges, X and Y are d-separated given the covariates:\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid z) \\quad \\text{if } (Y \\perp X \\mid Z)_{G_{\\overline{X}}}\n\n\n\n\n\n\n\n\nNote 1: These formulas are flexible\n\n\n\nWith all three of these rules, the variables names are interchangeable depending on the relationships in the DAG: X, Y, and Z can stand for any sets of variables. Additionally, the conditioning set (the z term) could be empty (e.g. Z \\rightarrow Y with no other nodes involved).\nSimilarly, notice how the (Y \\perp X \\mid Z)_{G_{\\underline{X}}} in Simplified Rule 2 looks slightly different than the (Y \\perp Z \\mid X, W)_{G_{\\overline{X}\\underline{Z}}} in Rule 2. That’s because the variable names swap roles. In the general rule, Z is a placeholder for whatever variable is being exchanged, but in the simplified version, we’re exchanging \\operatorname{do}(x), so we use X. Additionally, Z (the confounder) replaces the general W conditioning set, and there are no other existing interventions, so the G_{\\overline{X}} modification disappears."
  },
  {
    "objectID": "do-calculus.html#backdoor-adjustment",
    "href": "do-calculus.html#backdoor-adjustment",
    "title": "do-calculus",
    "section": "Backdoor adjustment",
    "text": "Backdoor adjustment\nLet’s say that we’re interested in the causal effect of X on Y, but we have a confounder Z that opens a backdoor path X \\leftarrow Z \\rightarrow Y, creating a DAG that looks like this:\n\n\n\n\n\n\n\n\nFigure 4: DAG with a confounder\n\n\n\n\n\nOur target estimand is P(y \\mid \\operatorname{do}(x)),1 or the distribution of Y in a world where we intervene to set X = x. However, that \\operatorname{do}(\\cdot) operator represents a hypothetical intervention where we can set X = x directly, breaking its usual causes. In an experimental setting, that works fine—we can randomly assign people to get assigned to different values of x. But in observational data, we don’t have control over treatment assignment and we cannot intervene.\nWe can use the simplified rules of do-calculus to eliminate the \\operatorname{do}(x) and estimate the causal effect from observational data.\n\nDeriving the backdoor formula\nWith the rules of do-calculus, we can take our estimand of interest—the effect of X on Y, or P(y \\mid \\operatorname{do}(x))—and transform it into a do-free statement that deals with the confounding from Z.\n\nStep 1: Incorporate Z into P(y \\mid \\operatorname{do}(x))\nTo adjust for the confounder Z, we need to incorporate it into the formula for our estimand. We do this by considering the joint distribution of Y and Z in a world where X occurs through an intervention, or P(y, z \\mid \\operatorname{do}(x)). This gives us the probability that Y = y and Z = z simultaneously when we force X = x.\nTo get our target P(y \\mid \\operatorname{do}(x)) from this joint distribution, we marginalize over Z, or sum over all possible values of Z:\n\n{\\color{blue} P(y \\mid \\operatorname{do}(x))} = \\sum_z {\\color{red} P(y, z \\mid \\operatorname{do}(x))}\n\\tag{1}\nOur estimand now includes Z, but this joint distribution equation is hard to work with. We need to break it smaller, more manageable pieces and separate y and x. Using the chain rule of probability (which applies to interventional distributions just as it does to observational ones), we can expand out Equation 1:\n\n{\\color{red} P(y, z \\mid \\operatorname{do}(x))} = {\\color{orange} P(y \\mid \\operatorname{do}(x), z) \\times P(z \\mid \\operatorname{do}(x))}\n\\tag{2}\nWe can substitute that expanded version back into Equation 1:\n\n{\\color{blue} P(y \\mid \\operatorname{do}(x))} = \\sum_z {\\color{orange} P(y \\mid \\operatorname{do}(x), z) \\times P(z \\mid \\operatorname{do}(x))}\n\\tag{3}\nOr without colors:\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid \\operatorname{do}(x), z) \\times P(z \\mid \\operatorname{do}(x))\n\\tag{4}\nNow we have two \\operatorname{do}(\\cdot)-based quantities that we cannot directly estimate from data:\n\nP(y \\mid \\operatorname{do}(x), z): “Given that we intervene on X and observe Z = z, what’s the probability of Y = y?”\nP(z \\mid \\operatorname{do}(x)): “In a world where we intervene on X, what’s the probability that Z = z?”\n\nOur next job is to use the rules of do-calculus to remove those \\operatorname{do}(\\cdot) interventions.\n\n\nStep 2: Applying Rule 2 to P(y \\mid \\operatorname{do}(x), z)\nAccording to Simplified Rule 2, we can replace an interventional \\operatorname{do}(x) with a regular observed x if we meet specific conditions in G_{\\underline{X}}, or the graph with all arrows out of X deleted: Y must be d-separated from X, given the covariates Z. Here’s what G_{\\underline{X}} looks like:\n\n\n\n\n\n\n\n\nFigure 5: Check d-separation of X and Y given Z in G_{\\underline{X}}\n\n\n\n\n\nThe only path between X and Y in G_{\\underline{X}} is X \\leftarrow Z \\rightarrow Y. Adjusting for Z blocks this path, so (Y \\perp X \\mid Z)_{G_{\\underline{X}}} holds. That means that we can swap out \\operatorname{do}(x) for a regular observed x:\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid x, z)\n\n\n\nStep 3: Applying Rule 3 to P(z \\mid \\operatorname{do}(x))\nAccording to Simplified Rule 3, we can remove a \\operatorname{do}(\\cdot) operator if we meet specific conditions in G_{\\overline{X}}, or the graph with all arrows into X deleted: Z must be d-separated from X in G_{\\overline{X}}.\n\n\n\n\n\n\nNoteWait, this doesn’t exactly match Rule 3??\n\n\n\nSimplified Rule 3 officially talks about independence between X and Y, but here we’re talking about X and Z. What gives?\nRemember from Tip 1 that these variable names are flexible. We don’t have to look only at X and Y—any nodes can stand in for those. In this case, we care about the relationship between Z and X, where the “outcome” variable is Z instead of Y.\nAdditionally, technically Simplified Rule 3 includes conditioning set z: P(y \\mid \\operatorname{do}(x), z). However, the z term in the formula can be empty. In this case, Y is related to X and Z as a collider and, accordingly, we don’t adjust for it, so we can leave it out of the equation. Thus, we can take this official expression from Rule 3:\n\nP(y \\mid \\operatorname{do}(x), z)\n\nand modify it by\n\nswitching the “outcome” variable to Z, so y becomes z, and\nusing an empty conditioning set, so the “z” in P(y \\mid \\operatorname{do}(x), z) disappears\n\n…resulting in\n\nP(z \\mid \\operatorname{do}(x))\n\nfor this special case.\n\n\n\n\n\n\n\n\n\n\nFigure 6: Check d-separation of Z and X in G_{\\overline{X}}\n\n\n\n\n\nThe only path between Z and X in G_{\\overline{X}} is Z \\rightarrow Y \\leftarrow X. In this case, Y is a collider and since we don’t adjust for or condition on colliders, that pathway is blocked and (Z \\perp X)_{G_{\\overline{X}}} holds. That means that we can completely eliminate \\operatorname{do}(x):\n\nP(z \\mid \\operatorname{do}(x)) = P(z)\n\n\n\nStep 4: Final formula\nFinally we can substitute both do-free results back into our original expression from Equation 4:\n\n\\begin{aligned}\nP(y \\mid \\operatorname{do}(x)) &= \\sum_z \\underbrace{P(y \\mid \\operatorname{do}(x), z)}_{\\text{Rule 2}} \\times \\underbrace{P(z \\mid \\operatorname{do}(x))}_{\\text{Rule 3}} \\\\\n&= \\sum_z \\underbrace{P(y \\mid x, z)}_{\\text{Rule 2}} \\times \\underbrace{P(z)}_{\\text{Rule 3}}\n\\end{aligned}\n\nThis gives us the official backdoor adjustment formula:\n\n\\boxed{\\rule{0pt}{1.5em}\\;\\; P(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid x, z) \\times P(z) \\;\\;}\n\\tag{5}\nAll \\operatorname{do}(\\cdot) operators are gone and every term on the right-hand side is an ordinary observational value. This means that we can estimate the causal effect of X on Y from observational data as long as we measure and adjust for Z."
  },
  {
    "objectID": "do-calculus.html#front-door-adjustment",
    "href": "do-calculus.html#front-door-adjustment",
    "title": "do-calculus",
    "section": "Front-door adjustment",
    "text": "Front-door adjustment\nThe backdoor criterion requires that we can observe and adjust for all confounders. But what if the confounder is unobserved? It turns out that if we can observe a mediator Z that fully mediates X’s effect on Y, we can still identify the causal effect using the front-door criterion. This derivation is a little more complex, but still doable with the rules of do-calculus.\nConsider a DAG where:\n\nX causes Y entirely through a mediator Z (no direct X \\rightarrow Y edge)\nThere is an unobserved confounder U that affects both X and Y\nU does not affect Z directly\n\n\n\n\n\n\n\n\n\nFigure 7: Front-door DAG: X affects Y only through Z, but an unobserved confounder U creates a backdoor path\n\n\n\n\n\nBecause U is unobserved, we cannot adjust for it—it’s a confounder, but we cannot use backdoor adjustment. But we can get around that by using a front-door approach.\n\nDeriving the front-door formula\nOur target estimand is again P(y \\mid \\operatorname{do}(x)). Unlike the backdoor case, the simplified rules are not sufficient here—we need the full rules, which handle multiple simultaneous interventions and combined graph modifications like G_{\\overline{X}\\underline{Z}}.\n\nStep 1: Marginalize over Z\nAs with the backdoor derivation, we start by incorporating Z into our estimand. We marginalize over Z and expand using the chain rule:\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid \\operatorname{do}(x), z) \\times P(z \\mid \\operatorname{do}(x))\n\\tag{6}\nWe now need to eliminate that interventional \\operatorname{do}(x) from both terms.\n\n\nStep 2: Simplify P(z \\mid \\operatorname{do}(x)) with Rule 2\nRule 2 says we can replace \\operatorname{do}(x) with observing x if Z and X are d-separated in the graph with arrows out of X removed.\n\n\n\n\n\n\n\n\nFigure 8: Check d-separation of Z and X in G_{\\\\underline{X}}\n\n\n\n\n\nIn G_{\\underline{X}}, the only path between X and Z goes through U: X \\leftarrow U \\rightarrow Y \\leftarrow Z. But Y is a collider on this path and we don’t condition on it, so the path is blocked. Therefore (Z \\perp X)_{G_{\\underline{X}}} holds, and:\n\nP(z \\mid \\operatorname{do}(x)) = P(z \\mid x)\n\n\n\nStep 3: Simplify P(y \\mid \\operatorname{do}(x), z)\nThis term is trickier. We need to replace \\operatorname{do}(x) with something we can work with. The strategy is to first convert the observation of z into an intervention \\operatorname{do}(z), then drop \\operatorname{do}(x), and finally eliminate \\operatorname{do}(z) via backdoor adjustment.\nStep 3a: Apply Rule 2 (in reverse) to convert observation z to \\operatorname{do}(z).\nRule 2 tells us that an intervention \\operatorname{do}(z) can be replaced with an observation z (or vice versa) if a d-separation condition holds. Here we want to go from observing z to intervening \\operatorname{do}(z), so we check the condition for the reverse direction (because Rule 2 is an equality, it’s true in either direction, so we can look at it either way):\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid \\operatorname{do}(x), \\operatorname{do}(z)) \\quad \\text{if } (Y \\perp Z \\mid X)_{G_{\\overline{X}\\underline{Z}}}\n\nWe need to verify that Y and Z are d-separated given X in the graph where we:\n\nRemove arrows into X (the \\overline{X} modification)\nRemove arrows out of Z (the \\underline{Z} modification)\n\n\n\n\n\n\n\n\n\nFigure 9: Check d-separation of Y and Z given X in G_{\\\\overline{X}\\\\underline{Z}}\n\n\n\n\n\nIn G_{\\overline{X}\\underline{Z}}, the remaining edges are X \\rightarrow Z and U \\rightarrow Y (since U \\rightarrow X is removed by \\overline{X} and Z \\rightarrow Y is removed by \\underline{Z}). There is no path connecting Z and Y at all, so (Y \\perp Z \\mid X)_{G_{\\overline{X}\\underline{Z}}} trivially holds. We can make the exchange:\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid \\operatorname{do}(x), \\operatorname{do}(z))\n\nSince Rule 2 is an equivalence (P(y \\mid \\operatorname{do}(x), \\operatorname{do}(z), w) = P(y \\mid \\operatorname{do}(x), z, w) when the condition holds), it works symmetrically—if the condition holds, we can replace an observation with an intervention just as easily as the other way around.\nStep 3b: Apply Rule 3 to drop \\operatorname{do}(x).\nRule 3 says we can remove an intervention \\operatorname{do}(x) if a d-separation condition holds in a specially modified graph:\n\nP(y \\mid \\operatorname{do}(x), \\operatorname{do}(z)) = P(y \\mid \\operatorname{do}(z)) \\quad \\text{if } (Y \\perp X \\mid Z, W)_{G_{\\overline{Z}\\overline{X(W)}}}\n\nIn our expression P(y \\mid \\operatorname{do}(x), \\operatorname{do}(z)), there are no observed conditioning variables, so W = \\varnothing. Since W is empty, no node in X can be an ancestor of a W-node, meaning X(W) = X—all nodes in X qualify for having their incoming arrows removed. The graph we check is therefore G_{\\overline{Z}\\overline{X}}, with incoming arrows into both Z and X removed:\n\n\n\n\n\n\n\n\nFigure 10: Check d-separation of Y and X in G_{\\\\overline{Z}\\\\overline{X}}\n\n\n\n\n\nIn G_{\\overline{Z}\\overline{X}}, the edges U \\rightarrow X and X \\rightarrow Z are both removed. X is completely isolated—no edges connect it to any other node—so X and Y are trivially d-separated regardless of conditioning. Therefore (Y \\perp X \\mid Z)_{G_{\\overline{Z}\\overline{X}}} holds, and by Rule 3:\n\nP(y \\mid \\operatorname{do}(x), \\operatorname{do}(z)) = P(y \\mid \\operatorname{do}(z))\n\nBy combining steps 3a and 3b, we get this:\n\nP(y \\mid \\operatorname{do}(x), z) = P(y \\mid \\operatorname{do}(z))\n\nStep 3c: Eliminate \\operatorname{do}(z) via backdoor adjustment.\nNow we need to turn P(y \\mid \\operatorname{do}(z)) into purely observational terms. In the original DAG, there is a backdoor path from Z to Y: Z \\leftarrow X \\leftarrow U \\rightarrow Y. We can block this by adjusting for X (which is observed). Conditioning on and marginalizing over X:\n\nP(y \\mid \\operatorname{do}(z)) = \\sum_{x'} P(y \\mid z, x') \\times P(x')\n\n\n\n\n\n\n\nNoteWhy can we adjust for X here?\n\n\n\nWhen estimating the effect of Z on Y, X acts as a confounder (it causes Z and, through U, is associated with Y). Conditioning on X blocks the backdoor path Z \\leftarrow X \\leftarrow U \\rightarrow Y. This works even though U is unobserved—X is sufficient because it intercepts the chain between U and Z.\nMore formally, we apply Rule 2 to get P(y \\mid \\operatorname{do}(z), x') = P(y \\mid z, x') (since X blocks all backdoor paths from Z to Y), and Rule 3 to get P(x' \\mid \\operatorname{do}(z)) = P(x') (since there is no directed path from Z to X).\n\n\n\n\nStep 4: Final formula\nFinally, we can substitute everything back into Equation 6:\n\n\\begin{aligned}\nP(y \\mid \\operatorname{do}(x)) &= \\sum_z \\underbrace{P(y \\mid \\operatorname{do}(x), z)}_{\\text{Step 3}} \\times \\underbrace{P(z \\mid \\operatorname{do}(x))}_{\\text{Step 2}} \\\\\n&= \\sum_z \\underbrace{\\sum_{x'} P(y \\mid z, x') \\times P(x')}_{\\text{Backdoor on } Z \\to Y} \\times \\underbrace{P(z \\mid x)}_{\\text{Rule 2}}\n\\end{aligned}\n\nThis gives us the official front-door adjustment formula:\n\n\\boxed{\\rule{0pt}{1.5em}\\;\\; P(y \\mid \\operatorname{do}(x)) = \\sum_z P(z \\mid x) \\sum_{x'} P(y \\mid x', z) \\times P(x') \\;\\;}\n\\tag{7}\nAll \\operatorname{do}(\\cdot) operators are gone and term on the right-hand side is an ordinary observational quantity. We can then use this in two steps:\n\nP(z \\mid x): Estimate how X affects Z. This is unconfounded because U doesn’t directly affect Z.\n\\sum_{x'} P(y \\mid x', z) \\times P(x'): Estimate how Z affects Y, adjusting for X to block the backdoor path through U.\n\nBy chaining these two unconfounded estimates together, we recover the full causal effect of X on Y despite never observing U."
  },
  {
    "objectID": "do-calculus.html#footnotes",
    "href": "do-calculus.html#footnotes",
    "title": "do-calculus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStrictly speaking, that’s not actually true! P(y \\mid \\operatorname{do}(x)) is an interventional distribution, while causal effects or estimands are contrasts of that distribution, like E[Y \\mid \\operatorname{do}(x = 1)] - E[Y \\mid \\operatorname{do}(x = 0)]. But for the sake of simplicity, we’ll pretend.↩︎"
  },
  {
    "objectID": "colliders.html",
    "href": "colliders.html",
    "title": "Colliders",
    "section": "",
    "text": "Colliders\n\n\n\nd3 = require(\"d3@7\")\njStat = require(\"jstat@1.9.6\")\ndag = import(new URL(\"js/dag-utils.js\", document.baseURI).href)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships between nodes\n\nviewof strength_xz = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-z\"&gt;Z&lt;/span&gt; strength`\n})\n\nviewof strength_yz = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; → &lt;span class=\"node node-z\"&gt;Z&lt;/span&gt; strength`\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationship between X and Y\n\nviewof xy_exists = Inputs.toggle({\n  label: html`&lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; exists (&lt;em&gt;true causal effect&lt;/em&gt;)`\n})\n\nviewof strength_xy = Inputs.range([0, 1], {\n  value: 0.3, \n  step: 0.05,\n  label: html`&lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; strength`\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustments\n\nviewof adjust_z = Inputs.toggle({\n  label: html`&lt;span class=\"node node-z\"&gt;Adjust for Z&lt;/span&gt; (&lt;em&gt;or \"condition on\"&lt;/em&gt; &lt;span class=\"node node-z\"&gt;Z&lt;/span&gt;)`\n})\n\n\n\n\n\n\n\n// ----------------\n// Status readout\n// ----------------\n{\n  const trueEffect = xy_exists ? strength_xy : 0;\n\n  // Helper functions for formatting true negative signs\n  const fmt_number = (x, d = 3) =&gt;\n    (x &lt; 0 ? \"\\u2212\" : \"\") + Math.abs(x).toFixed(d);\n  const fmt_signed = (x, d = 3) =&gt;\n    (x &gt;= 0 ? \"+\" : \"\\u2212\") + Math.abs(x).toFixed(d);\n  \n  const bias = slope_cond - slope_all;\n\n  return html`&lt;div class=\"alert alert-secondary status-readout\"&gt;\n    &lt;h5 class=\"alert-heading\"&gt;Observed effects&lt;/h5&gt;\n    &lt;table&gt;\n      &lt;tr&gt;\n        &lt;td&gt;True &lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; effect&lt;/td&gt;\n        &lt;td&gt;${trueEffect === 0 ? \"none\" : fmt_number(trueEffect, 2)}&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;Overall slope&lt;/td&gt;\n        &lt;td&gt;${fmt_number(slope_all)}&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr class=\"summary ${adjust_z ? '' : 'dimmed'}\"&gt;\n        &lt;td&gt;Apparent slope (conditioning on &lt;span class=\"node node-z\"&gt;Z&lt;/span&gt;)&lt;/td&gt;\n        &lt;td&gt;${adjust_z ? fmt_number(slope_cond) : \"—\"}&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;N (selected / total)&lt;/td&gt;\n        &lt;td&gt;${adjust_z ? html`${n_selected} / ${simData.length}` : html`${simData.length} / ${simData.length}`}&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr class=\"${adjust_z ? '' : 'dimmed'}\"&gt;\n        &lt;td&gt;Bias introduced&lt;/td&gt;\n        &lt;td&gt;${adjust_z\n          ? html`&lt;span class=\"${Math.abs(bias) &gt; 0.05 ? 'text-danger' : ''}\"&gt;${fmt_signed(bias)}&lt;/span&gt;`\n          : \"—\"\n        }&lt;/td&gt;\n      &lt;/tr&gt;\n    &lt;/table&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\n\n\n\n\n// Javascript doesn't have native seed functions like set.seed(), so we make one here\n_seed = {\n  // Mulberry32 seeded PRNG\n  function mulberry32(a) {\n    return () =&gt; {\n      a |= 0; a = a + 0x6D2B79F5 | 0;\n      let t = Math.imul(a ^ a &gt;&gt;&gt; 15, 1 | a);\n      t = t + Math.imul(t ^ t &gt;&gt;&gt; 7, 61 | t) ^ t;\n      return ((t ^ t &gt;&gt;&gt; 14) &gt;&gt;&gt; 0) / 4294967296;\n    }\n  }\n\n  const rng = mulberry32(674751);  // From random.org\n  const N = 500;\n  const randn = (sd = 1) =&gt;\n    jStat.normal.inv(rng() * 0.998 + 0.001, 0, sd);\n\n  return {\n    xVals: Array.from({ length: N }, () =&gt; randn()),\n    noiseY: Array.from({ length: N }, () =&gt; randn()),\n    noiseZ: Array.from({ length: N }, () =&gt; randn(0.5))\n  };\n}\n\n\n\n\n\n\n\n// ----------------\n// Simulated data\n// ----------------\n// Z is binary: 1 if the latent combination exceeds 0\nsimData = {\n  const { xVals, noiseY, noiseZ } = _seed;\n  const N = xVals.length;\n  const beta = xy_exists ? strength_xy : 0;\n\n  return xVals.map((x, i) =&gt; {\n    const y = beta * x + noiseY[i];\n    const z_latent = strength_xz * x + strength_yz * y + noiseZ[i];\n    const z = z_latent &gt; 0 ? 1 : 0;\n    return { x, y, z, group: z === 1 ? \"Z = 1\" : \"Z = 0\" };\n  });\n}\n\n\n\n\n\n\n\nz1_points = simData.filter(d =&gt; d.z === 1)\nz0_points = simData.filter(d =&gt; d.z === 0)\nselected = adjust_z ? z1_points : simData\nn_selected = selected.length\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction ols_slope(data) {\n  if (data.length &lt; 3) return 0;\n  const xs = data.map(d =&gt; d.x);\n  const ys = data.map(d =&gt; d.y);\n  const r = jStat.corrcoeff(xs, ys);\n  return r * jStat.stdev(ys, true) / jStat.stdev(xs, true);\n}\n\nslope_all = ols_slope(simData)\nslope_cond = ols_slope(selected)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// -----------------\n// Interactive DAG\n// -----------------\n{\n  const width = 600;\n  const height = 250;\n  const nodeRadius = 36;\n\n  const nodes = {\n    X: { x: 130, y: 200, label: \"X\" },\n    Z: { x: width / 2, y: 60, label: \"Z\" },\n    Y: { x: 470, y: 200, label: \"Y\" }\n  };\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", `0 0 ${width} ${height}`)\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .style(\"max-width\", \"100%\");\n\n  const defs = svg.append(\"defs\");\n\n  dag.addArrowMarkers(defs);\n\n  // Arrows\n  const edges = [\n    {\n      id: \"xz\", from: nodes.X, to: nodes.Z,\n      strength: strength_xz, blocked: false\n    },\n    {\n      id: \"yz\", from: nodes.Y, to: nodes.Z,\n      strength: strength_yz, blocked: false\n    }\n  ];\n\n  if (xy_exists) {\n    edges.push({\n      id: \"xy\", from: nodes.X, to: nodes.Y,\n      strength: strength_xy, blocked: false\n    });\n  }\n\n  for (const edge of edges) {\n    dag.drawEdge(svg, edge, nodeRadius);\n  }\n\n  // Add highlighted area behind X and Y when adjusting for Z\n  if (adjust_z) {\n    const padX = 8;\n    const padY = 10;\n    const left = nodes.X.x - nodeRadius - padX;\n    const right = nodes.Y.x + nodeRadius + padX;\n    const top = nodes.X.y - nodeRadius - padY;\n    const h = (nodeRadius + padY) * 2;\n    const w = right - left;\n\n    svg.append(\"rect\")\n      .attr(\"x\", left)\n      .attr(\"y\", top)\n      .attr(\"width\", w)\n      .attr(\"height\", h)\n      .attr(\"rx\", h / 2)\n      .attr(\"ry\", h / 2)\n      .attr(\"fill\", dag.colorZ)\n      .attr(\"opacity\", 0.18);\n  }\n\n  // Nodes\n  dag.drawSolidNode(\n    svg, nodes.X.x, nodes.X.y, nodeRadius, dag.colorX\n  );\n\n  dag.drawSolidNode(\n    svg, nodes.Y.x, nodes.Y.y, nodeRadius, dag.colorY\n  );\n\n  // Z: solid gold, semi-transparent when not conditioning,\n  // fully opaque when conditioning\n  dag.drawSolidNode(\n    svg, nodes.Z.x, nodes.Z.y, nodeRadius, dag.colorZ,\n    adjust_z ? 1 : 0.35\n  );\n\n  // Labels\n  for (const n of Object.values(nodes)) {\n    dag.drawLabel(svg, n.x, n.y, n.label);\n  }\n\n  return svg.node();\n}\n\n\n\n\n\n\n\n// Scatterplot of points with regression line(s)\nPlot.plot({\n  width: 550,\n  height: 340,\n  style: { fontSize: \"12px\" },\n  x: { label: \"X\" },\n  y: { label: \"Y\" },\n  color: {\n    domain: [\"Z = 0\", \"Z = 1\"],\n    range: [dag.colorZ0, dag.colorZ]\n  },\n  marks: [\n    // Faded excluded points (Z = 0 when conditioning)\n    adjust_z\n      ? Plot.dot(z0_points, {\n          x: \"x\", y: \"y\",\n          fill: dag.colorZ0,\n          r: 3,\n          fillOpacity: 0.2\n        })\n      : null,\n\n    // Active points\n    Plot.dot(selected, {\n      x: \"x\", y: \"y\",\n      fill: \"group\",\n      r: 4,\n      fillOpacity: 0.75,\n      stroke: \"#fff\",\n      strokeWidth: 0.5\n    }),\n\n    // Overall regression line\n    Plot.linearRegressionY(simData, {\n      x: \"x\", y: \"y\",\n      stroke: \"#8b8b99\",\n      strokeWidth: 2\n    }),\n\n    // Conditioned regression line\n    adjust_z\n      ? Plot.linearRegressionY(z1_points, {\n          x: \"x\", y: \"y\",\n          stroke: dag.apparentLine,\n          strokeWidth: 2.5,\n          strokeDasharray: \"8 5\"\n        })\n      : null\n  ].filter(Boolean)\n})"
  },
  {
    "objectID": "adjustment.html",
    "href": "adjustment.html",
    "title": "How to adjust",
    "section": "",
    "text": "Conditioning can be explicit, like including a variable in a model. This is often called “adjusting.”\nConditioning can also be implicit, like collecting data only about one part of the population, which sneakily builds it into the data and can create collider bias."
  },
  {
    "objectID": "adjustment.html#backdoor-adjustment",
    "href": "adjustment.html#backdoor-adjustment",
    "title": "How to adjust",
    "section": "Backdoor adjustment",
    "text": "Backdoor adjustment\nBackdoor adjusting involves removing the effect of confounders Z to estimate the average causal effect of a treatment X on an outcome Y.\nInstead of working with abstract Xs, Ys, and Zs, we’ll make things more concrete with a more realistic example of the effect of mosquito nets on household health status:1\n\nX = mosquito net use (binary: 0 = no net, 1 = uses net)\nY = health status (continuous score, roughly 0–100)\nZ = household income (binary: 0 = low, 1 = high)\n\nIncome confounds the relationship between nets and health: wealthier households are more likely to use nets and tend to be healthier for other reasons.\n\nShow code for tikz-based DAG\n\\begin{tikzpicture}[&gt;={stealth}]\n  \\node (x) at (0,0) {X};\n  \\node (y) at (2,0) {Y};\n  \\node (z) at (1,1) {Z};\n  \\path[-&gt;] (z) edge (x);\n  \\path[-&gt;] (z) edge (y);\n  \\path[-&gt;] (x) edge (y);\n\\end{tikzpicture}\n\n\nShow code for {ggdag}-based DAG\nlibrary(ggdag)\n\nmosquito_dag &lt;- dagify(\n  Y ~ X + Z,\n  X ~ Z,\n  exposure = \"X\",\n  outcome = \"Y\",\n  coords = list(x = c(X = 1, Y = 3, Z = 2), y = c(X = 1, Y = 1, Z = 2)),\n  labels = c(X = \"Mosquito net\", Y = \"Health status\", Z = \"Household income\")\n)\n\nmosquito_dag_plot &lt;- mosquito_dag |&gt;\n  tidy_dagitty() |&gt;\n  node_status()\n\nggplot(mosquito_dag_plot, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(family = \"Atkinson Hyperlegible\") +\n  geom_dag_label(\n    data = filter(mosquito_dag_plot, !is.na(status)),\n    aes(label = label),\n    nudge_y = -0.35,\n    family = \"Atkinson Hyperlegible\"\n  ) +\n  geom_dag_label(\n    data = filter(mosquito_dag_plot, is.na(status)),\n    aes(label = label),\n    nudge_y = 0.35,\n    family = \"Atkinson Hyperlegible\"\n  ) +\n  scale_color_manual(\n    values = c(\"#9b332b\", \"#262d42\"),\n    na.value = \"#d39a2d\",\n    guide = \"none\"\n  ) +\n  scale_x_continuous(limits = c(0.7, 3.3)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\nFigure 1: Basic DAG showing that household income (Z) confounds the relationship between mosquito net use (X) and health status (Y)\n\n\n\n\nSimulated data and naive estimate\nTo illustrate all these different adjustment techniques, we’ll simulate some data where the true causal effect of net use on health is 10 points.\nPeople have a 50% chance of having high or low income. Net use depends on income, and wealthier households are more likely to use nets. Health depends on both net use and income, plus some random noise.\n\nlibrary(tidyverse)\nlibrary(parameters)\nlibrary(marginaleffects)\n\nn &lt;- 1000\n\nwithr::with_seed(114941, {\n  net_data &lt;- tibble(\n    z = rbinom(n, 1, 0.5),\n    x = rbinom(n, 1, plogis(-1 + 2 * z)),\n    y = 50 + 10 * x + 15 * z + rnorm(n, 0, 5)\n  ) |&gt;\n    select(x, y, z)\n})\n\nIf we just look at the relationship of X on Y without adjusting for anything, we will not get the correct effect for X because of confounding. The coefficient on x here is bigger than the true effect of 10 because it conflates the causal effect of nets with the income advantage that net users tend to have.\n\nmodel_naive &lt;- lm(y ~ x, data = net_data)\nmodel_parameters(model_naive, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       53.77 | 0.36 | [53.07, 54.47] | 151.29 | &lt; .001\nx           |       17.32 | 0.51 | [16.32, 18.32] |  34.05 | &lt; .001\n\n\n\n\nHow do we isolate what we’re looking for?\nThe main estimand we care about here is the causal effect of mosquito nets on health status, or what would happen if we intervened on X and set X = x (or gave people nets).\nUsing do-calculus notation, we’ll work with this interventional distribution:\n\n\\begin{aligned}\n&P(y \\mid \\operatorname{do}(x)), \\text{ or} \\\\[6pt]\n&P(\\text{Health} \\mid \\operatorname{do}(\\text{Net}))\n\\end{aligned}\n\nThe specific quantity we’ll estimate is a comparison or contrast within this distribution—we want to know the difference in the average value of Y when we intervene and when we don’t intervene with nets, or:\n\n\\begin{aligned}\n&E[Y \\mid \\operatorname{do}(x = 1)] - E[Y \\mid \\operatorname{do}(x = 0)], \\text{ or} \\\\[6pt]\n&E[\\text{Health} \\mid \\operatorname{do}(\\text{net} = 1)] - E[\\text{Health} \\mid \\operatorname{do}(\\text{net} = 0)]\n\\end{aligned}\n\nThe backdoor adjustment formula tells us what we need to do to estimate P(y \\mid \\operatorname{do}(x)):\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_z P(y \\mid x, z) \\times P(z)\n\nIn words, this means we need to estimate the distribution of Y given X and Z (P(y \\mid x, z)) weighted by the marginal probability of each level of Z (P(z)).\n\n\nHow to actually close backdoors\nThere are many ways to implement backdoor adjustment, like these common approaches:\n\nStratification\nRegression adjustment\nMatching\nInverse probability weighting\nG-computation\n\nEach of these methods estimates the distribution of P(y \\mid x, z) weighted by P(z), which lets us estimate P(y \\mid \\operatorname{do}(x)) with only observational data.\nI’ll show a short example of each of these below, and I’ll include some extra resources and information about them in each section. There are are different variations and flavors of these different approaches, and there are methods I don’t include here2—this is definitely not comprehensive!\n\nStratification\nThe most direct translation of the backdoor formula is to hold Z constant by stratifying, or finding the effect of net use within each income group and calculating the weighted average across groups.\nWe can do this with group_by() and summarize() from {dplyr}:\n\ngroup_effects &lt;- net_data |&gt;\n  group_by(z) |&gt; \n  summarize(\n    effect = mean(y[x == 1]) - mean(y[x == 0]),\n    n = n()\n  )\ngroup_effects\n\n# A tibble: 2 × 3\n      z effect     n\n  &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n1     0   10.1   506\n2     1   10.3   494\n\nweighted.mean(group_effects$effect, group_effects$n)\n\n[1] 10.18125\n\n\nStratification is neat and easy, but only works with discrete confounders that have few levels. If you have a continuous confounder (e.g., actual monthly income instead of a binary yes/no), or a confounder with lots of levels (e.g., country or state), or multiple confounders (e.g. both income and country), you’d have too many groups and too few observations per group.\n\n\nRegression adjustment\nAn alternative to direct stratified weighted averages is to use regression (which is fundamentally just fancy averaging). We can include the confounder as a covariate:\n\n# Control for z\nmodel_controlled &lt;- lm(y ~ x + z, data = net_data)\nmodel_parameters(model_controlled, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(997) |      p\n-------------------------------------------------------------------\n(Intercept) |       50.03 | 0.23 | [49.57, 50.49] | 213.78 | &lt; .001\nx           |       10.18 | 0.35 | [ 9.49, 10.88] |  28.78 | &lt; .001\nz           |       14.63 | 0.35 | [13.94, 15.33] |  41.36 | &lt; .001\n\n\nThe coefficient for x is the effect of net use on health, holding income constant. You can include any number of confounders and they can be binary, categorical, or continuous.\nRegression adjustment works well when the functional form is correct. In this case, we simulated the data using linear relationships without any interactions or non-linear terms, so the causal effect is correct. But in real life, you’re betting that the functional form of the model captures the true relationship between confounders and outcome.\n\n\nMatching\n\n\n\n\n\n\nNoteExtra resources\n\n\n\nTODO\n\n\nAnother approach is to modify the sample before fitting an outcome model so that we can compare treated and untreated units without confounding.\nWe can match each treated unit with a comparable control unit based on their values of the confounder (e.g. create a new sample where net users and non-users are paired within each income group).\n{MatchIt} makes this straightforward, and it provides a ton of different matching methods and estimands. Here we’ll use exact matching:\n\nlibrary(MatchIt)\n\n# Find matched pairs of x based on values of z\nm &lt;- matchit(x ~ z, data = net_data, method = \"exact\", estimand = \"ATE\")\n\n# Extract the matches as a data frame\nmatched_data &lt;- match_data(m)\n\n# Estimate y ~ x with the matched data (and use the matched weights)\nmodel_matched &lt;- lm(y ~ x, data = matched_data, weights = weights)\nmodel_parameters(model_matched, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.24 | 0.39 | [56.47, 58.00] | 147.11 | &lt; .001\nx           |       10.18 | 0.56 | [ 9.09, 11.27] |  18.28 | &lt; .001\n\n\nThe coefficient for x is the effect of net use on health. Because we estimated this on matched data that has comparable values of the confounder, we’ve adjusted for Z.\n\n\nInverse probability weighting (IPW)\n\n\n\n\n\n\nNoteExtra resources\n\n\n\nTODO\n\n\nMatching is neat, but it typically involves discarding unmatched units. An alternative approach is to reweight observations to create balanced groups of treated and untreated units.\nThe most common way to do this is to create inverse probability weights, where all treated people get a weight of \\frac{1}{p_i} and all untreated people get a weight of \\frac{1}{1 - p_i} (where p_i refers to each person’s propensity scores). This creates a “pseudo-population” where treatment is independent of the confounders, mimicking what randomization would have achieved.\nPractically speaking, this gives more weight to more unusual and unexpected observations. For instance, someone with a high predicted probability (90%) of choosing to use a net who then uses a net isn’t really notable and will have a low weight (\\frac{1}{0.9}, or 1.111). Someone else with a high probability (90% again) of choosing a net who doesn’t use a net will have a high weight (\\frac{1}{1 - 0.9}, or 10). This makes it so that the should-have-probably-used-a-net person is more comparable to the corresponding treated people.\nWe adjust for confounding with inverse probability weights with this process:\n\nCreate a treatment model: Use the confounders to calculate propensity scores—or the probability of being treated—for each observation, typically using logistic regression (if the treatment is binary) or some other regression model (if the treatment is continuous)\nCalculate inverse probability weights: Use the propensity scores and observed treatment status to create weights. There are lots of ways to do this! A common formula for binary treatments looks like this:\n\n\\frac{\\text{Treatment}}{\\text{Propensity}} + \\frac{1 - \\text{Treatment}}{1 - \\text{Propensity}}\n\nThe {WeightIt} package provides many other weighting methods for binary and continuous treatments.\nUse the weights in an outcome model: Use the inverse probability weights to reweight all the observations in your sample when running your regression model. A common way to do this is with the weights argument in lm().\n\n\nManual IPWIPW with {WeightIt}\n\n\n\n# Treatment model with logistic regression\n# Estimate probability of x based on values of z\nmodel_ipw_treatment &lt;- glm(\n  x ~ z,\n  family = binomial(link = \"logit\"),\n  data = net_data\n)\n\n# Propensity scores and weights\ndata_ipw &lt;- net_data |&gt;\n  mutate(\n    prob = predict(model_ipw_treatment, type = \"response\"),\n    ipw = (x / prob) + ((1 - x) / (1 - prob))\n  )\n\n# Outcome model\n# Estimate y ~ x with the newly weighted pseudo-population\nmodel_ipw_outcome &lt;- lm(y ~ x, data = data_ipw, weights = ipw)\nmodel_parameters(model_ipw_outcome, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.24 | 0.39 | [56.47, 58.01] | 145.37 | &lt; .001\nx           |       10.18 | 0.56 | [ 9.09, 11.27] |  18.28 | &lt; .001\n\n\nTechnically the coefficient is correct, but the standard errors do not account for the fact that the weights were estimated, so they will be incorrect.\nOne way around this is to calculate bootstrapped standard errors; see Chapter 2 and Chapter 11 in Causal Inference in R [@todo] for more details and complete examples.\n\n\n\nlibrary(WeightIt)\n\n# Treatment model with logistic regression\n# Estimate probability of x based on values of z\nW &lt;- weightit(x ~ z, data = net_data, method = \"glm\", estimand = \"ATE\")\n\n# Outcome model\n# Estimate y ~ x with the newly weighted pseudo-population\nmodel_ipw_weightit &lt;- lm_weightit(y ~ x, data = net_data, weightit = W)\nmodel_parameters(model_ipw_weightit, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.24 | 0.34 | [56.57, 57.91] | 167.81 | &lt; .001\nx           |       10.18 | 0.35 | [ 9.49, 10.88] |  28.76 | &lt; .001\n\n\n\n\n\n\n\n\nNoteUse {WeightIt} weights manually\n\n\n\nYou don’t have to use lm_weightit() for the outcome model. You can extract the actual weights from W and use them in regular lm(), but then you’re responsible for making sure the standard errors are correctly adjusted, which is tricky. By default, lm_weightit() internally uses something called “M-estimation” to account for the fact that the weights themselves are uncertain. If we use something like vcov = \"HC0\" in model_parameters(), we’ll get robust standard errors without incorporating the uncertainty in the weights, and they’ll be different:\n\n# Add propensity scores and weights to data\ndata_ipw_weightit &lt;- net_data |&gt;\n  mutate(\n    prob = W$ps,\n    ipw = W$weights\n  )\n\n# Outcome model\n# Estimate y ~ x with the newly weighted pseudo-population\nmodel_ipw_weightit_manual &lt;- lm(y ~ x, data = data_ipw_weightit, weights = ipw)\nmodel_parameters(\n  model_ipw_weightit_manual,\n  vcov = \"HC0\",\n  verbose = FALSE\n)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       57.24 | 0.45 | [56.36, 58.11] | 128.25 | &lt; .001\nx           |       10.18 | 0.64 | [ 8.93, 11.43] |  15.97 | &lt; .001\n\n\nOne way around this is to calculate bootstrapped standard errors; see Chapter 11 in Causal Inference in R [@todo] for more details. You can also calculate bootstrapped standard errors with lm_weightit() if you want with lm_weightit(..., vcov = \"BS\").\n\n\n\n\n\nThe coefficient for x is the effect of net use on health. Because we estimated this on a reweighted pseudo-population, we’ve adjusted for Z.\n\n\nG-computation\n\n\n\n\n\n\nNoteExtra resources\n\n\n\nTODO\n\n\nMatching and inverse probability weighting both let us manipulate the sample to get balanced populations—exactly balanced in the case of matching, reweighted pseudo-populations in the case of IPW. With these approaches, we model the process of being assigned to treatment.\nA neat alternative to weighting and sample manipulation is to use something called the parametric G-formula or G-computation. It’s surprisingly straightforward:\n\nFit a model that estimates the outcome, controlling for confounders\nUse the model to generate a set of predictions where we pretend that every row was treated (i.e. set x = 1)\nUse the model to generate a set of predictions where we pretend that every row was not treated (i.e. set x = 0)\nCalculate the difference in the two sets of predictions, then find the average of that. This is the causal effect.\n\nThis approach feels more like the causal estimand that we originally defined:\n\n\\begin{aligned}\n&E[Y \\mid \\operatorname{do}(x = 1)] - E[Y \\mid \\operatorname{do}(x = 0)], \\text{ or} \\\\[6pt]\n&E[\\text{Health} \\mid \\operatorname{do}(\\text{net} = 1)] - E[\\text{Health} \\mid \\operatorname{do}(\\text{net} = 0)]\n\\end{aligned}\n\nIn this case, we use code to set everyone’s treatment to \\operatorname{do}(\\text{net} = 1), then set everyone to \\operatorname{do}(\\text{net} = 0), and then find the average difference in outcomes.\nWith G-computation, we can technically use whatever kind of outcome model we want. We can use a simple model like lm(y ~ x + z), or we can include interactions and splines or use even more flexible machine learning models to reduce the risk of misspecification.\n\nManualWith {marginaleffects}IPW + G-computation\n\n\nThis manual approach works well, but doesn’t include standard errors or other details. You can calculate standard errors by bootstrapping—see Chapter 13 in Causal Inference in R [@todo]—or you use something like {marginaleffects}.\n\n# Fit outcome model with confounders\nmodel_gcomp &lt;- lm(y ~ x + z, data = net_data)\n\n# Predict outcomes when everyone is treated and everyone is untreated\ny1 &lt;- predict(model_gcomp, newdata = mutate(net_data, x = 1))\ny0 &lt;- predict(model_gcomp, newdata = mutate(net_data, x = 0))\n\n# Find difference in means, or average treatment effect (ATE)\nmean(y1) - mean(y0)\n\n[1] 10.18308\n\n\n\n\n{marginaleffects} can do all the work of setting everyone’s treatment to 0, to 1, and finding the difference automatically. It can also provide robust and/or clustered standard errors.\n\nmodel_gcomp &lt;- lm(y ~ x + z, data = net_data)\n\navg_comparisons(model_gcomp, variables = list(x = 0:1))\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     10.2      0.354 28.8   &lt;0.001 602.5  9.49   10.9\n\nTerm: x\nType: response\nComparison: 1 - 0\n\n\n\n\nWe technically don’t have to use an unweighted outcome model. We can use IPW like normal and then use it for g-computation. The authors of {WeightIt} recommend using weighted g-computation whenever possible to benefit from the idea of pseudo-populations and the flexibility of calculating contrasts.\n\n# Treatment model with logistic regression\n# Estimate probability of x based on values of z\nW_gcomp &lt;- weightit(x ~ z, data = net_data, method = \"glm\", estimand = \"ATE\")\n\n# Outcome model\n# Estimate y ~ x with the newly weighted pseudo-population\n# Including x * z allows the effect of x to vary across levels of z\nmodel_weighted &lt;- lm_weightit(y ~ x * z, data = net_data, weightit = W_gcomp)\n\n# G-computation on the weighted model\navg_comparisons(model_weighted, variables = list(x = 0:1))\n\n\n Estimate Std. Error    z Pr(&gt;|z|)     S 2.5 % 97.5 %\n     10.2      0.354 28.8   &lt;0.001 601.8  9.49   10.9\n\nTerm: x\nType: probs\nComparison: 1 - 0"
  },
  {
    "objectID": "adjustment.html#front-door-adjustment",
    "href": "adjustment.html#front-door-adjustment",
    "title": "How to adjust",
    "section": "Front-door adjustment",
    "text": "Front-door adjustment\n\n\n\n\n\n\nNoteExtra resources\n\n\n\nTODO\n\n\nAll the methods above require observing the confounder Z. But what if there are unobservable confounders that we can’t measure?!\nLet’s change the example a little bit. We’ll get rid of Z and add two new nodes:\n\nX = mosquito net use (binary: 0 = no net, 1 = uses net)\nY = health status (continuous score, roughly 0–100)\nM = average number of mosquitos near the bed at night (continuous)\nU = unobserved general health behaviors\n\nMosquitos (M) are the mechanism for how mosquito nets affect health status. When nets are used, the number of mosquitos present in the immediate vicinity of a person should drop significantly. In theory, that count of mosquitos should be completely uninfluenced by anything else and only influenced by mosquito net use. In practice, this probably isn’t the case! Actual front-door mediators are really hard to find in real life. But just go with it.\nGeneral health behaviors (U) confound the relationship between nets and health, but we can’t measure them; health-conscious people are more likely to use nets and tend to be healthier through other channels.\n\nShow code for tikz-based DAG\n\\definecolor{ucolor}{HTML}{888888}\n\n\\begin{tikzpicture}[&gt;={stealth}]\n  \\node (x) at (0,0) {X};\n  \\node (m) at (2,0) {M};\n  \\node (y) at (4,0) {Y};\n  \\node[ucolor] (u) at (2,1.5) {U};\n  \\path[-&gt;] (x) edge (m);\n  \\path[-&gt;] (m) edge (y);\n  \\path[-&gt;, dashed, ucolor] (u) edge (x);\n  \\path[-&gt;, dashed, ucolor] (u) edge (y);\n\\end{tikzpicture}\n\n\nShow code for {ggdag}-based DAG\nmosquito_dag_frontdoor &lt;- dagify(\n  Y ~ M + U,\n  X ~ U,\n  M ~ X,\n  exposure = \"X\",\n  outcome = \"Y\",\n  latent = \"U\",\n  coords = list(\n    x = c(X = 1, Y = 3, M = 2, U = 2),\n    y = c(X = 1, Y = 1, M = 1, U = 2)\n  ),\n  labels = c(\n    X = \"Mosquito net\",\n    Y = \"Health status\",\n    M = \"Mosquitos near bed\",\n    U = \"Health behaviors\"\n  )\n)\n\nmosquito_dag_frontdoor_plot &lt;- mosquito_dag_frontdoor |&gt;\n  tidy_dagitty() |&gt;\n  node_status() |&gt; \n  mutate(arrow_color = case_when(\n    name != \"U\" ~ \"black\",\n    name == \"U\" ~ \"grey80\"\n  ))\n\ndag_fd &lt;- ggplot(\n  mosquito_dag_frontdoor_plot,\n  aes(x = x, y = y, xend = xend, yend = yend)\n) +\n  geom_dag_edges(aes(edge_color = arrow_color)) +\n  geom_dag_point(aes(color = status)) +\n  geom_dag_text(family = \"Atkinson Hyperlegible\") +\n  geom_dag_label(\n    data = filter(\n      mosquito_dag_frontdoor_plot,\n      status != \"latent\" | is.na(status)\n    ),\n    aes(label = label),\n    nudge_y = -0.35,\n    family = \"Atkinson Hyperlegible\"\n  ) +\n  geom_dag_label(\n    data = filter(mosquito_dag_frontdoor_plot, status == \"latent\"),\n    aes(label = label),\n    nudge_y = 0.35,\n    family = \"Atkinson Hyperlegible\"\n  ) +\n  scale_color_manual(\n    values = c(\"#9b332b\", \"grey50\", \"#262d42\"),\n    na.value = \"#d39a2d\",\n    guide = \"none\"\n  ) +\n  scale_x_continuous(limits = c(0.69, 3.31)) +\n  theme_void()\ndag_fd\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\nFigure 2: More complex DAG showing that unobserved health behaviors (U) confound the relationship between mosquito net use (X) and health status (Y), which only occurs through the number of mosquitos near a bed (M)\n\n\n\n\nSimulated data and naive estimate\nAs before, we’ll simulate some data for this new DAG:\n\nn &lt;- 1000\n\nwithr::with_seed(915623, {\n  net_data_fd &lt;- tibble(\n    u = rnorm(n),\n    x = rbinom(n, 1, plogis(-0.5 + 1.5 * u)),\n    m = 10 - 5 * x + rnorm(n, 0, 2),\n    y = 70 - 2 * m + 4 * u + rnorm(n, 0, 3)\n  ) |&gt;\n    # Throw away u so we can't use it\n    select(x, y, m)\n})\n\nIf we just look at the relationship of X on Y without adjusting for U, we will not get the correct effect for X because of confounding. The coefficient on x here is bigger than the true effect of 10 because it conflates the causal effect of nets with health behaviors.\n\nmodel_naive_fd &lt;- lm(y ~ x, data = net_data_fd)\nmodel_parameters(model_naive_fd, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       48.04 | 0.25 | [47.56, 48.53] | 194.11 | &lt; .001\nx           |       14.00 | 0.40 | [13.22, 14.79] |  35.01 | &lt; .001\n\n\n\n\nHow do we isolate what we’re looking for?\nEven though we used a column named u in the simulated dataset, we got rid of it. We have no observable confounders, which means that we can’t use backdoor adjustment.\nWe can, however, use front-door adjustment! The front-door adjustment formula tells us what we need to do to estimate P(y \\mid \\operatorname{do}(x)):\n\nP(y \\mid \\operatorname{do}(x)) = \\sum_m P(m \\mid x) \\sum_{x'} P(y \\mid x', m) \\times P(x')\n\n\nP(m \\mid x): Estimate how X affects M. This is unconfounded because U doesn’t directly affect M.\n\\sum_{x'} P(y \\mid x', m) \\times P(x'): Estimate how M affects Y, adjusting for X to block the backdoor path through U.\n\nBy chaining these two unconfounded estimates together, we can find the full causal effect of X on Y despite never observing U.\n\n\nHow to actually do front-door adjustment\n\nEffect of X on M\nFirst we need to find P(m \\mid x), or the effect of X on M.\n\n\nShow code\n# Dig into the guts of a ggplot object to add a new layer and move it to the\n# back - this is needed because I want the annotate() highlight area to\n# appear behind the nodes, not on top\nprepend_layer &lt;- function(plot, ...) {\n  p &lt;- plot + ...\n  n &lt;- length(p$layers)\n  p$layers &lt;- p$layers[c(n, 1:(n - 1))]\n  p\n}\n\ndag_fd |&gt;\n  prepend_layer(\n    annotate(\n      geom = \"rect\",\n      xmin = 0.795,\n      xmax = 2.205,\n      ymin = 0.775,\n      ymax = 1.225,\n      color = \"#41485f\",\n      fill = \"#f7c267\",\n      alpha = 0.5\n    )\n  )\n\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\nThere’s no confounding on this path—U doesn’t control mosquito access to the bed—so we can use a simple regression model:\n\nmodel_fd_xm &lt;- lm(m ~ x, data = net_data_fd)\nmodel_parameters(model_fd_xm, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(998) |      p\n-------------------------------------------------------------------\n(Intercept) |       10.00 | 0.08 | [ 9.85, 10.16] | 123.14 | &lt; .001\nx           |       -5.07 | 0.13 | [-5.32, -4.81] | -38.58 | &lt; .001\n\n# Extract the coefficient for x:\ncoef_xm &lt;- model_fd_xm |&gt; \n  model_parameters(keep = \"x\", verbose = FALSE) |&gt; \n  pull(Coefficient)\ncoef_xm\n\n[1] -5.065073\n\n\nIt looks like using a mosquito net reduces the number of mosquitos near the bed by −5.1 on average. Nets work!\n\n\nEffect of M on Y, adjusting for X\nNext we need to find \\sum_{x'} P(y \\mid x', m) \\times P(x'), or the effect of M on Y, adjusting for X.\n\n\nShow code\ndag_fd |&gt;\n  prepend_layer(\n    annotate(\n      geom = \"rect\",\n      xmin = 0.795 + 1,\n      xmax = 2.205 + 1,\n      ymin = 0.775,\n      ymax = 1.225,\n      color = \"#41485f\",\n      fill = \"#f7c267\",\n      alpha = 0.5\n    )\n  )\n\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\nThe path M \\leftarrow X \\leftarrow U \\rightarrow Y is a backdoor from M to Y, but we can block it by adjusting for X. U is actually the true source of confounding, but since it’s unobserved, we can block the path by conditioning on X instead. We’ll just use regression adjustment here (but we could theoretically use any other method, like IPW, matching, G-computation, etc.)\n\nmodel_fd_my &lt;- lm(y ~ m + x, data = net_data_fd)\nmodel_parameters(model_fd_my, verbose = FALSE)\n\nParameter   | Coefficient |   SE |         95% CI | t(997) |      p\n-------------------------------------------------------------------\n(Intercept) |       68.02 | 0.75 | [66.54, 69.50] |  90.38 | &lt; .001\nm           |       -2.00 | 0.07 | [-2.14, -1.85] | -27.40 | &lt; .001\nx           |        3.89 | 0.48 | [ 2.95,  4.82] |   8.15 | &lt; .001\n\n# Extract the coefficient for m:\ncoef_my &lt;- model_fd_my |&gt; \n  model_parameters(keep = \"m\", verbose = FALSE) |&gt; \n  pull(Coefficient)\ncoef_my\n\n[1] -1.996825\n\n\nFrom this, it looks like increasing the number of mosquitos near the bed by one causes a −2.0 point decrease in health status, on average. Mosquitos are bad!\n\n\nCombined effect\nTo find the total causal effect of X on Y, we multiply the two effects together:\n\ncoef_xm * coef_my\n\n[1] 10.11407\n\n\nPerfect! The effect of nets on health status is the same as what we built into the data, but we estimated it without touching U. This works because M fully transmits the causal effect of X and there is no unobserved confounding of the M \\rightarrow Y relationship except through X, letting us decompose the problem into two identifiable pieces.\n\n\nShow code\ndag_fd |&gt;\n  prepend_layer(\n    annotate(\n      geom = \"rect\",\n      xmin = 0.795,\n      xmax = 2.205 + 1,\n      ymin = 0.775,\n      ymax = 1.225,\n      color = \"#41485f\",\n      fill = \"#f7c267\",\n      alpha = 0.5\n    )\n  )\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\nThis multiplication works because the relationships are linear. In more complex situations like non-linear models or models with interaction terms, we would need to apply the full front‑door formula by averaging predictions, similar to G‑computation.\n\n\n\nHic sunt dracones\nSituations like this are exceptionally rare in practice:\n\nThe mediator must be completely unaffected by the unobserved confounder U (i.e., no arrow from U \\rightarrow M). This is an incredibly strong assumption. In this situation, it means that there cannot be any way that an unobserved confounder could possibly influence the number of mosquitos in the immediate vicinity of someone’s bed—the only possible reason for mosquitos near the bed is the presence or absence of a net. That’s… implausible.\nThe mediator must also fully mediate the effect of X on Y; there can be no direct path from X \\rightarrow Y. Here that means that mosquito nets can only influence health status because they reduce the number of mosquitos near you while sleeping at night. This is a little more plausible—most people tend to only use nets at night while they sleep.\n\nIf these assumptions hold, front‑door adjustment is a powerful tool; if not, the estimate will be biased."
  },
  {
    "objectID": "adjustment.html#footnotes",
    "href": "adjustment.html#footnotes",
    "title": "How to adjust",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI created this example on a whim back in 2019 when I taught my first causal inference class, and it’s proven really useful. I use it all my causal inference classes, in several blog posts, and it’s even used as an example throughout Chapter 2 of Causal Inference in R [@todo].↩︎\nLike marginal structural models and structural nested mean models [@todo1] or doubly robust augmented inverse probability weighting [@todo2]. You can get really fancy with these techniques.↩︎"
  },
  {
    "objectID": "confounders.html",
    "href": "confounders.html",
    "title": "Confounders",
    "section": "",
    "text": "Confounders\n\n\n\nd3 = require(\"d3@7\")\ndag = import(new URL(\"js/dag-utils.js\", document.baseURI).href)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelationships between nodes\n\nviewof strength_zx = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-z\"&gt;Z&lt;/span&gt; → &lt;span class=\"node node-x\"&gt;X&lt;/span&gt; strength`\n})\n\nviewof strength_zy = Inputs.range([0, 1], {\n  value: 0.5, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-z\"&gt;Z&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; strength`\n})\n\nviewof strength_xy = Inputs.range([0, 1], {\n  value: 0.7, \n  step: 0.05, \n  label: html`&lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; strength`\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdjustments\n\nviewof adjust_z = Inputs.toggle({\n  label: html`&lt;span class=\"node node-z\"&gt;Adjust for Z&lt;/span&gt; (&lt;em&gt;block backdoor path&lt;/em&gt;)`\n})\n\n\n\n\n\n\n\n// ----------------\n// Status readout\n// ----------------\n{\n  const pctPure = Math.round(y_pure_x / yMax * 100);\n  const pctConf = Math.round(y_confounded / yMax * 100);\n  const pctZ = Math.round(y_direct_z / yMax * 100);\n  const pctOwn = Math.max(0, 100 - pctPure - pctConf - pctZ);\n\n  return html`&lt;div class=\"alert alert-secondary status-readout\"&gt;\n    &lt;h5 class=\"alert-heading\"&gt;What Y contains&lt;/h5&gt;\n    &lt;table&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorX}\"/&gt;&lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-x\"&gt;X&lt;/span&gt;'s direct influence on &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt;&lt;/td&gt;\n        &lt;td&gt;${pctPure}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;\n          &lt;defs&gt;\n            &lt;pattern id=\"legend-hatch-conf\" patternUnits=\"userSpaceOnUse\"\n              width=\"6\" height=\"6\" patternTransform=\"rotate(45)\"&gt;\n              &lt;rect width=\"6\" height=\"6\" fill=\"${dag.colorZ}\"/&gt;\n              &lt;line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"6\"\n                stroke=\"${dag.colorX}\" stroke-width=\"2.5\"/&gt;\n            &lt;/pattern&gt;\n          &lt;/defs&gt;\n          &lt;rect width=\"12\" height=\"12\" fill=\"url(#legend-hatch-conf)\"/&gt;\n        &lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-z\"&gt;Z&lt;/span&gt;'s influence on &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; via &lt;span class=\"node node-x\"&gt;X&lt;/span&gt;&lt;/td&gt;\n        &lt;td&gt;${pctConf}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorZ}\"/&gt;&lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-z\"&gt;Z&lt;/span&gt;'s direct influence on &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt;&lt;/td&gt;\n        &lt;td&gt;${pctZ}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr&gt;\n        &lt;td&gt;&lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorY}\"/&gt;&lt;/svg&gt;&lt;/td&gt;\n        &lt;td&gt;&lt;span class=\"node node-y\"&gt;Y&lt;/span&gt;'s own variation&lt;/td&gt;\n        &lt;td&gt;${pctOwn}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr class=\"summary ${adjust_z ? 'dimmed' : ''}\"&gt;\n        &lt;td&gt;\n          &lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorX}\"/&gt;&lt;/svg&gt;\n          +\n          &lt;svg width=\"12\" height=\"12\"&gt;\n            &lt;rect width=\"12\" height=\"12\" fill=\"url(#legend-hatch-conf)\"/&gt;\n          &lt;/svg&gt;\n        &lt;/td&gt;\n        &lt;td&gt;Apparent &lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; effect&lt;/td&gt;\n        &lt;td&gt;${pctPure + pctConf}%&lt;/td&gt;\n      &lt;/tr&gt;\n      &lt;tr class=\"summary ${adjust_z ? '' : 'dimmed'}\"&gt;\n        &lt;td&gt;\n          &lt;svg width=\"12\" height=\"12\"&gt;&lt;rect width=\"12\" height=\"12\" fill=\"${dag.colorX}\"/&gt;&lt;/svg&gt;\n        &lt;/td&gt;\n        &lt;td&gt;Unconfounded &lt;span class=\"node node-x\"&gt;X&lt;/span&gt; → &lt;span class=\"node node-y\"&gt;Y&lt;/span&gt; effect&lt;/td&gt;\n        &lt;td&gt;${pctPure}%&lt;/td&gt;\n      &lt;/tr&gt;\n    &lt;/table&gt;\n  &lt;/div&gt;`;\n}\n\n\n\n\n\n\n\n\n\n\nyMax = 150\nbaseVal = 50\n\nx_from_z = adjust_z ? 0 : strength_zx * baseVal\n\ny_pure_x = strength_xy * (adjust_z ? 1 : (1 - strength_zx)) * baseVal\ny_confounded = adjust_z ? 0 : strength_xy * x_from_z\ny_direct_z = adjust_z ? 0 : strength_zy * baseVal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// -----------------\n// Interactive DAG\n// -----------------\n{\n  const width = 600;\n  const height = 250;\n  const nodeRadius = 36;\n\n  const nodes = {\n    Z: { x: width / 2, y: 60, label: \"Z\" },\n    X: { x: 130, y: 200, label: \"X\" },\n    Y: { x: 470, y: 200, label: \"Y\" }\n  };\n\n  const svg = d3.create(\"svg\")\n    .attr(\"viewBox\", `0 0 ${width} ${height}`)\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .style(\"max-width\", \"100%\");\n\n  const defs = svg.append(\"defs\");\n\n  dag.addArrowMarkers(defs);\n  dag.addHatchPattern(\n    defs, \"hatch-confounded\", dag.colorZ, dag.colorX, 45\n  );\n  dag.addCircleClip(\n    defs, \"x-clip\", nodes.X.x, nodes.X.y, nodeRadius\n  );\n  dag.addCircleClip(\n    defs, \"y-clip\", nodes.Y.x, nodes.Y.y, nodeRadius\n  );\n\n  // Arrows\n  const edges = [\n    {\n      id: \"zx\", from: nodes.Z, to: nodes.X,\n      strength: strength_zx, blocked: adjust_z\n    },\n    {\n      id: \"zy\", from: nodes.Z, to: nodes.Y,\n      strength: strength_zy, blocked: adjust_z\n    },\n    {\n      id: \"xy\", from: nodes.X, to: nodes.Y,\n      strength: strength_xy, blocked: false\n    }\n  ];\n\n  for (const edge of edges) {\n    dag.drawEdge(svg, edge, nodeRadius);\n  }\n\n  // Nodes\n  // X: strength directly controls fill proportion\n  dag.drawNode(svg, nodes.X.x, nodes.X.y, nodeRadius, \"x-clip\", {\n    bottomUp: [],\n    topDown: [\n      { prop: adjust_z ? 0 : strength_zx, fill: dag.colorZ }\n    ]\n  }, undefined, dag.colorX);\n\n  // Y: blue base, incoming effects overlay\n  dag.drawNode(svg, nodes.Y.x, nodes.Y.y, nodeRadius, \"y-clip\", {\n    bottomUp: [\n      { prop: Math.min(y_pure_x / yMax, 1), fill: dag.colorX },\n      {\n        prop: Math.min(y_confounded / yMax, 1),\n        fill: \"url(#hatch-confounded)\"\n      }\n    ],\n    topDown: [\n      { prop: Math.min(y_direct_z / yMax, 1), fill: dag.colorZ }\n    ]\n  }, undefined, dag.colorY);\n\n  dag.drawSolidNode(\n    svg, nodes.Z.x, nodes.Z.y, nodeRadius, dag.colorZ\n  );\n\n  // Labels\n  for (const n of Object.values(nodes)) {\n    dag.drawLabel(svg, n.x, n.y, n.label);\n  }\n\n  return svg.node();\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interactive DAGs",
    "section": "",
    "text": "blah blah blah"
  }
]